{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Conv2D, BatchNormalization, ReLU, Add, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Function to check for corrupted images\n",
    "def check_image(file_path):\n",
    "    try:\n",
    "        # Check if the image can be opened with PIL\n",
    "        with Image.open(file_path) as img:\n",
    "            img.verify()  # Verify that image is not corrupted\n",
    "\n",
    "        # Check if the image can be opened with cv2\n",
    "        img_cv2 = cv2.imread(file_path)\n",
    "        if img_cv2 is None:\n",
    "            raise IOError(\"Cannot open image with cv2\")\n",
    "        return True\n",
    "\n",
    "    except (IOError, SyntaxError, cv2.error) as e:\n",
    "        print(f'Removing corrupted image {file_path}: {e}')\n",
    "        return False\n",
    "\n",
    "# Function to remove corrupted images\n",
    "def remove_corrupted_images(directory):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            if not check_image(file_path):\n",
    "                os.remove(file_path)\n",
    "                print(f'Removed corrupted image: {file_path}')\n",
    "\n",
    "# Paths to your directories\n",
    "train_dir = r\"D:\\Development\\Dataset\\Bone_Fracture\\train\"\n",
    "test_dir = r\"D:\\Development\\Dataset\\Bone_Fracture\\test\"\n",
    "val_dir = r\"D:\\Development\\Dataset\\Bone_Fracture\\val\"\n",
    "\n",
    "# Clean the directories\n",
    "remove_corrupted_images(train_dir)\n",
    "remove_corrupted_images(test_dir)\n",
    "remove_corrupted_images(val_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom generator to handle image loading errors\n",
    "class SafeDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, generator):\n",
    "        self.generator = generator\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.generator)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_x, batch_y = self.generator[index]\n",
    "        safe_batch_x = []\n",
    "        safe_batch_y = []\n",
    "\n",
    "        for img, label in zip(batch_x, batch_y):\n",
    "            try:\n",
    "                # Check if the image can be opened with PIL\n",
    "                img_pil = Image.fromarray((img * 255).astype('uint8'))\n",
    "                img_pil.verify()\n",
    "                safe_batch_x.append(img)\n",
    "                safe_batch_y.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping image due to error: {e}\")\n",
    "\n",
    "        if len(safe_batch_x) == 0:\n",
    "            raise ValueError(\"All images in the batch are invalid.\")\n",
    "\n",
    "        return np.array(safe_batch_x), np.array(safe_batch_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(128,128),\n",
    "    batch_size=20,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(128,128),\n",
    "    batch_size=20\n",
    ")\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(128,128),\n",
    "    batch_size=20,\n",
    "    class_mode='binary',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap original generators with SafeDataGenerator\n",
    "train_generator_safe = SafeDataGenerator(train_generator)\n",
    "val_generator_safe = SafeDataGenerator(val_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet(x, filters, kernel_size=3, stride=1, use_shortcut=True):\n",
    "    shortcut = x\n",
    "    # First convolution layer\n",
    "    x = Conv2D(filters, kernel_size=kernel_size, strides=stride, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    if use_shortcut:\n",
    "        shortcut = Conv2D(filters, kernel_size=1, padding='same')(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    x = Add()([x, shortcut])\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(128,128,3))\n",
    "\n",
    "# Starting the model creation\n",
    "x = Conv2D(32, (3,3))(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "\n",
    "# Creating ResNet\n",
    "x = resnet(x, 16)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(32, activation='relu')(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    r'D:\\PROJECT\\Deep_Learning_Projects\\Fracture_Segmentation\\model_checkpoint.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator_safe,\n",
    "    steps_per_epoch=len(train_generator_safe),\n",
    "    validation_data=val_generator_safe,\n",
    "    validation_steps=len(val_generator_safe),\n",
    "    epochs=20,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
